# This is a sample Python script.

# Press Shift+F10 to execute it or replace it with your code.
# Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.
import math
import sys
import random
import matplotlib.pyplot as plt

import numpy as np

import numpy as np
beta = 0.9
lr = 0.1
def one_hot(y_list):
    y_new =[]
    for y in range(len(y_list)):
        yi = [0]*10
        yi[int(y_list[y])] = 1
        y_new.append(np.array(yi))
    return y_new

def bprop(fprop_cache):
  # Follows procedure given in notes
  x, y, z1, h1, z2, h2, loss = [fprop_cache[key] for key in ('x', 'y', 'z1', 'h1', 'z2', 'h2', 'loss')]
  dz2 = (h2 - y)                                #  dL/dz2
  dW2 = np.dot(dz2, h1.T)                       #  dL/dz2 * dz2/dw2
  db2 = dz2                                     #  dL/dz2 * dz2/db2
  dz1 = np.dot(fprop_cache['W2'].T,
    (h2 - y)) * sigmoid(z1) * (1-sigmoid(z1))   #  dL/dz2 * dz2/dh1 * dh1/dz1
  dW1 = np.dot(dz1, x.T)                        #  dL/dz2 * dz2/dh1 * dh1/dz1 * dz1/dw1
  db1 = dz1                                     #  dL/dz2 * dz2/dh1 * dh1/dz1 * dz1/db1
  return {'b1': db1, 'W1': dW1, 'b2': db2, 'W2': dW2}


def test_after_train(W, B):
    pass


def insert_w_b(W, B, in_x, dim_out):
    sd_w, sd_b = math.sqrt(6 / (in_x + dim_out)), math.sqrt(6 / (1 + dim_out)),
    w1 = np.random.uniform(-sd_w, sd_w, (in_x, dim_out))
    b1 = np.random.uniform(-sd_b, sd_b, (1, dim_out))
    W.append(np.array(w1))
    B.append(np.array(b1))


def softmax(x):
    """ applies softmax to an input x"""
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum()


sigmoid = lambda x: 1 / (1 + np.exp(-x))


def create_dims(layer_num):
    return [128 * layer_num]
    pass


def update_parameters(loss, dic_parameters, W, B):
    pass


def create_w_b():
    random.seed(42)
    W, B = [], []
    w1 = np.random.rand(128, 784) / np.sqrt(784)
    W.append(w1)
    b0 = np.zeros((128, 1)) / np.sqrt(784)
    B.append(b0)
    w2 = np.random.rand(10, 128) / np.sqrt(128)
    W.append(w2)
    b1 = np.zeros((10, 1)) / np.sqrt(128)
    B.append(b1)
    '''
    in_x = len(train_x[0])
    # dim of layer
    dim = 128
    sd = 0
   
    insert_w_b(W, B, in_x, dim)
    in_x = dim
    for i in range(1, layer_num - 2):
        insert_w_b(W, B, in_x, dim)
    insert_w_b(W, B, in_x, 10)
    return W, B
'''
    return W,B

def calculate_loss(W, train_x, train_y):
    pass


def forward_instance(x, W, B, layers_num):
    H, Z = [], []
    for i in range (0, layers_num-2):
        zi = np.dot(W[i], x) + B[i]
        if i != layers_num - 2:
            hi = sigmoid(zi)
        else:
            hi = softmax(zi)
        H.append(zi)
        Z.append(hi)
    return H, Z




def output_prediction(predictions):
    f1 = open('test_y', 'w')
    for i in range(0, len(predictions)):
        f1.write(str(predictions[i]) + "\n")
    f1.close()


def neural_network(train_x, train_y, ephocs, batches,batch_size):
    W, B = create_w_b()
    W1,W2,B1,B2 =[],[],[],[]
    for e in range(ephocs):
        Xconvert = []
        temp = list(zip(train_x, train_y))
        random.shuffle(temp)
        pxlist1, pylist1 = zip(*temp)
        for j in range(len(train_x)//batch_size):
            begin = j * batch_size
            end = min(begin + batch_size, len(train_x) - 1)
            if begin > end:
                continue
            X = pxlist1[begin:end]
            Y = pylist1[begin:end]
            m_batch = end - begin
            for index in range (begin, end):
                if index > len(train_x)-1:
                    break
                new_index = index-(200*j)
                print(index)
                x1 = sigmoid(np.dot(W[0], np.reshape(X[new_index],(784,1))) + B[0])
                x2 = softmax(np.dot(W[1], x1) + B[1])
                if e == 0:
                    if(index==4998):
                        print("hi")
                    print (new_index)
                    dz2 = (x2 - np.reshape(Y[new_index],(10,1)))  # dL/dz2
                    dW2 = np.dot(dz2, x1.T)  # dL/dz2 * dz2/dw2
                    db1 = dz2  # dL/dz2 * dz2/db2
                    dz1 = np.dot(W[1].T,
                                 (x2 - np.reshape(Y[new_index],(10,1)))) * x1 * (1 - x1)  # dL/dz2 * dz2/dh1 * dh1/dz1
                    dW1 = np.dot(dz1, np.reshape(X[new_index],(784,1)).T)  # dL/dz2 * dz2/dh1 * dh1/dz1 * dz1/dw1
                    db0 = dz1
                else:
                    dW1_old = dW1
                    dW2_old = dW2
                    db0_old = db0
                    db1_old = db1
                    dz2 = (x2 - Y[new_index])  # dL/dz2
                    dW2 = np.dot(dz2, x1.T)  # dL/dz2 * dz2/dw2
                    db1 = dz2  # dL/dz2 * dz2/db2
                    dz1 = np.dot(W[1].T,
                                 (x2 - Y[new_index])) * x1 * (1 - x1)  # dL/dz2 * dz2/dh1 * dh1/dz1
                    dW1 = np.dot(dz1, X[new_index])  # dL/dz2 * dz2/dh1 * dh1/dz1 * dz1/dw1
                    db0 = dz1
                    ## Using the past gradients to calculate the present gradients
                    dW1 = (beta * dW1_old + (1. - beta) * dW1)
                    db0 = (beta * db0_old + (1. - beta) * db0)
                    dW2 = (beta * dW2_old + (1. - beta) * dW2)
                    db1 = (beta * db1_old + (1. - beta) * db1)
                W1.append(dW1)
                W2.append(dW2)
                B1.append(db0)
                B2.append(db1)
            W[0] = W[0] - (1. / m_batch) * (sum(W1)) * lr
            B[0] = B[0] - (1. / m_batch) * (sum(db0)) * (lr)
            W[1] = W[1] - (1. / m_batch) * (sum(dW2)) * lr
            B[1] = B[1] - (1. / m_batch) * (sum(db1)) * (lr)
            W1=[]
            W2=[]
            B1=[]
            B2=[]


def mini_set():
    # parameters should be: train_x train_y
    training_examples, training_labels = sys.argv[1], sys.argv[2]
    taken = random.sample(range(1, 55001), 5000)
    taken.sort()
    print(taken)
    with open('train_x_short', 'w') as out1:
        i = 1
        f1 = open(training_examples)
        for line1 in f1:
            if i in taken:
                out1.write(line1)
            i += 1
        f1.close()
    with open('train_y_short', 'w') as out2:
        i = 1
        f2 = open(training_labels)
        for line2 in f2:
            if i in taken:
                out2.write(line2)
            i += 1
        f2.close()


def load_data(path_x, path_y):
    train_x = np.loadtxt(path_x)
    train_y = np.loadtxt(path_y)
    print(type(train_x[0]))
    '''
    for i in range (0,5000):
        plt.imshow(train_x[i].reshape(28, -1), cmap="gray")
        plt.show()
    '''
    return train_x, train_y


# Press the green button in the gutter to run the script.
if __name__ == '__main__':
    layer_num = 3
    mini_set()
    train_x, train_y = load_data("train_x_short", "train_y_short")
   # create_w_b(train_x, layer_num)
    train_y = one_hot(train_y)
    W, B = neural_network(train_x, train_y, 500, 1000,200)
# prediction = test_after_train(W, B)
# output_prediction([1, 2, 3, 4])
